---
title: "Candidate Speeches"
author: "David I. Crabtree"
date: "10/1/2022"
output: 
  html_document:
    toc: true
params:
  candidate: "Evo Morales"
  opponent: "mnr"
  
---

# Speech Data for `r params$candidate`

## Summary:
  

```{r Setup, include = FALSE}

library(tidyverse)
library(here)
library(tm)
library(SnowballC)
library(syuzhet)
library(lubridate)

theme_set(theme_minimal())
knitr::opts_chunk$set(
                      warning = FALSE,
                      message = FALSE)


```

```{r }

speeches <- read_csv(here("data", "CCD_text.csv"))

speeches <- speeches %>%
  rename(Speech = X7) %>%
  select(-X6)

# speeches %>%
#   group_by(Politician) %>%
#   count()

speeches <- speeches %>%
  filter(Politician == params$candidate)

speeches %>%
  group_by(year(Date)) %>%
  count()

```




```{r }
text <- Corpus(VectorSource(speeches$Speech))

#Replacing "/", "@" and "|" with space
to_space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
text <- tm_map(text, to_space, "/")
text <- tm_map(text, to_space, "@")
text <- tm_map(text, to_space, "\\|")
text <- tm_map(text, to_space, "<p>")
text <- tm_map(text, to_space, "will")
text <- tm_map(text, to_space, "can")
text <- tm_map(text, to_space, "also")
text <- tm_map(text, to_space, "must")
text <- tm_map(text, to_space, "thank")

# Convert the text to lower case
text <- tm_map(text, content_transformer(tolower))
# Remove numbers
text <- tm_map(text, removeNumbers)
# Remove english common stopwords
text <- tm_map(text, removeWords, stopwords("english"))

# Remove your own stop word
# specify your custom stopwords as a character vector
# text <- tm_map(text, removeWords, c("s", "company", "team")) 

# Remove punctuations
text <- tm_map(text, removePunctuation)
# Eliminate extra white spaces
text <- tm_map(text, stripWhitespace)

# Text stemming - which reduces words to their root form
text <- tm_map(text, stemDocument)


# Build a term-document matrix
text_dtm <- TermDocumentMatrix(text)
dtm_m <- as.matrix(text_dtm)
# Sort by decreasing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
freq_words <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 20 most frequent words
head(freq_words, 20)

freq_words %>%
  filter(word == "corrupt") %>%
  count()
```


## Text Correlations {.tabset}
### "Opponent"
```{r correlations}
findAssocs(text_dtm, terms = c(params$opponent), corlimit = 0.25)	
```

### "Rig"
```{r }
findAssocs(text_dtm, terms = c("rig"), corlimit = 0.25)	
```

### "Media"
```{r }
findAssocs(text_dtm, terms = c("media"), corlimit = 0.25)	
```

### "News"
```{r }
findAssocs(text_dtm, terms = c("news"), corlimit = 0.25)	
```

### "Elections"
```{r }
findAssocs(text_dtm, terms = c("elect"), corlimit = 0.25)	
```

### "The Courts"
```{r }
findAssocs(text_dtm, terms = c("court"), corlimit = 0.25)	
```


## Emotional Classification
```{r }
#Emotional Classification

sentiments <- get_nrc_sentiment(speeches$Speech)

speeches_sent <- cbind(speeches, sentiments)

DT::datatable(sentiments)

sentiments %>%
  summarize(Anger = sum(sentiments$anger) / sum(sentiments),
            Anticipation = sum(sentiments$anticipation) / sum(sentiments),
            Disgust = sum(sentiments$disgust) / sum(sentiments),
            Fear = sum(sentiments$fear) / sum(sentiments),
            Joy = sum(sentiments$joy) / sum(sentiments),
            Sadness = sum(sentiments$sadness) / sum(sentiments),
            Surprise = sum(sentiments$surprise) / sum(sentiments),
            Trust = sum(sentiments$trust) / sum(sentiments),
            Negative = sum(sentiments$negative) / sum(sentiments),
            Positive = sum(sentiments$positive) / sum(sentiments))


```


